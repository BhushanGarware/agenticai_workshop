{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Evaluating a LangChain Agent on Vertex AI Agent Engine (Prebuilt template)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Adapted from | [Public Repo](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/agent-engine/intro_agent_engine.ipynb) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Just like any Generative AI application, AI agents require thorough evaluation to ensure they perform reliably and effectively. This evaluation should happen both in real-time (online) and on large datasets of test cases (offline). Developers building agent applications face a significant challenge in evaluating their performance. Both subjective (human feedback) and objective (measurable metrics) evaluations are essential for building trust in agent behavior.\n",
        "\n",
        "This tutorial shows how to evaluate a first-party Agent Engine Agent using Vertex AI Gen AI Evaluation for agent evaluation.\n",
        "\n",
        "The tutorial uses the following Google Cloud services and resources:\n",
        "\n",
        "*  Vertex AI Gen AI Evaluation\n",
        "*  Vertex AI Agent Engine\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "* Build and deploy an agent using LangChain\n",
        "* Prepare Agent Evaluation dataset\n",
        "* Single tool usage evaluation\n",
        "* Trajectory evaluation\n",
        "* Response evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK and other required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --user --quiet \"google-cloud-aiplatform[agent_engines,evaluation,langchain]\" \\\n",
        "    \"langchain_google_vertexai\" \\\n",
        "    \"cloudpickle==3.0.0\" \\\n",
        "    \"pydantic>=2.10\" \\\n",
        "    \"requests\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. In Colab or Colab Enterprise, you might see an error message that says \"Your session crashed for an unknown reason.\" This is expected. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import os\n",
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "LOCATION =\"us-central1\"\n",
        "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type: \"string\", placeholder: \"[your-bucket-name]\", isTemplate: true}\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "\n",
        "\n",
        "EXPERIMENT_NAME = \"evaluate-re-agent\"  # @param {type:\"string\"}\n",
        "\n",
        "vertexai.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=LOCATION,\n",
        "    staging_bucket=BUCKET_URI,\n",
        "    experiment=EXPERIMENT_NAME,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5303c05f7aa6"
      },
      "source": [
        "## Import libraries\n",
        "\n",
        "Import tutorial libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fc324893334"
      },
      "outputs": [],
      "source": [
        "# General\n",
        "import random\n",
        "import string\n",
        "\n",
        "from IPython.display import HTML, Markdown, display\n",
        "\n",
        "# Build agent\n",
        "from google.cloud import aiplatform\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from vertexai import agent_engines\n",
        "\n",
        "# Evaluate agent\n",
        "from vertexai.preview.evaluation import EvalTask\n",
        "from vertexai.preview.evaluation.metrics import (\n",
        "    PointwiseMetric,\n",
        "    PointwiseMetricPromptTemplate,\n",
        "    TrajectorySingleToolUse,\n",
        ")\n",
        "from vertexai.preview.reasoning_engines import LangchainAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVnBDX54gz7j"
      },
      "source": [
        "## Define helper functions\n",
        "\n",
        "Initiate a set of helper functions to print tutorial results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSgWjMD_g1_v"
      },
      "outputs": [],
      "source": [
        "def get_id(length: int = 8) -> str:\n",
        "    \"\"\"Generate a uuid of a specified length (default=8).\"\"\"\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "def display_eval_report(eval_result: pd.DataFrame) -> None:\n",
        "    \"\"\"Display the evaluation results.\"\"\"\n",
        "    metrics_df = pd.DataFrame.from_dict(eval_result.summary_metrics, orient=\"index\").T\n",
        "    display(Markdown(\"### Summary Metrics\"))\n",
        "    display(metrics_df)\n",
        "\n",
        "    display(Markdown(f\"### Row-wise Metrics\"))\n",
        "    display(eval_result.metrics_table)\n",
        "\n",
        "\n",
        "def display_drilldown(row: pd.Series) -> None:\n",
        "    \"\"\"Displays a drill-down view for trajectory data within a row.\"\"\"\n",
        "\n",
        "    style = \"white-space: pre-wrap; width: 800px; overflow-x: auto;\"\n",
        "\n",
        "    if not (\n",
        "        isinstance(row[\"predicted_trajectory\"], list)\n",
        "        and isinstance(row[\"reference_trajectory\"], list)\n",
        "    ):\n",
        "        return\n",
        "\n",
        "    for predicted_trajectory, reference_trajectory in zip(\n",
        "        row[\"predicted_trajectory\"], row[\"reference_trajectory\"]\n",
        "    ):\n",
        "        display(\n",
        "            HTML(\n",
        "                f\"<h3>Tool Names:</h3><div style='{style}'>{predicted_trajectory['tool_name'], reference_trajectory['tool_name']}</div>\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if not (\n",
        "            isinstance(predicted_trajectory.get(\"tool_input\"), dict)\n",
        "            and isinstance(reference_trajectory.get(\"tool_input\"), dict)\n",
        "        ):\n",
        "            continue\n",
        "\n",
        "        for tool_input_key in predicted_trajectory[\"tool_input\"]:\n",
        "            print(\"Tool Input Key: \", tool_input_key)\n",
        "\n",
        "            if tool_input_key in reference_trajectory[\"tool_input\"]:\n",
        "                print(\n",
        "                    \"Tool Values: \",\n",
        "                    predicted_trajectory[\"tool_input\"][tool_input_key],\n",
        "                    reference_trajectory[\"tool_input\"][tool_input_key],\n",
        "                )\n",
        "            else:\n",
        "                print(\n",
        "                    \"Tool Values: \",\n",
        "                    predicted_trajectory[\"tool_input\"][tool_input_key],\n",
        "                    \"N/A\",\n",
        "                )\n",
        "        print(\"\\n\")\n",
        "    display(HTML(\"<hr>\"))\n",
        "\n",
        "\n",
        "def display_dataframe_rows(\n",
        "    df: pd.DataFrame,\n",
        "    columns: list[str] | None = None,\n",
        "    num_rows: int = 3,\n",
        "    display_drilldown: bool = False,\n",
        ") -> None:\n",
        "    \"\"\"Displays a subset of rows from a DataFrame, optionally including a drill-down view.\"\"\"\n",
        "\n",
        "    if columns:\n",
        "        df = df[columns]\n",
        "\n",
        "    base_style = \"font-family: monospace; font-size: 14px; white-space: pre-wrap; width: auto; overflow-x: auto;\"\n",
        "    header_style = base_style + \"font-weight: bold;\"\n",
        "\n",
        "    for _, row in df.head(num_rows).iterrows():\n",
        "        for column in df.columns:\n",
        "            display(\n",
        "                HTML(\n",
        "                    f\"<span style='{header_style}'>{column.replace('_', ' ').title()}: </span>\"\n",
        "                )\n",
        "            )\n",
        "            display(HTML(f\"<span style='{base_style}'>{row[column]}</span><br>\"))\n",
        "\n",
        "        display(HTML(\"<hr>\"))\n",
        "\n",
        "        if (\n",
        "            display_drilldown\n",
        "            and \"predicted_trajectory\" in df.columns\n",
        "            and \"reference_trajectory\" in df.columns\n",
        "        ):\n",
        "            display_drilldown(row)\n",
        "\n",
        "\n",
        "def plot_bar_plot(\n",
        "    eval_result: pd.DataFrame, title: str, metrics: list[str] = None\n",
        ") -> None:\n",
        "    fig = go.Figure()\n",
        "    data = []\n",
        "\n",
        "    summary_metrics = eval_result.summary_metrics\n",
        "    if metrics:\n",
        "        summary_metrics = {\n",
        "            k: summary_metrics[k]\n",
        "            for k, v in summary_metrics.items()\n",
        "            if any(selected_metric in k for selected_metric in metrics)\n",
        "        }\n",
        "\n",
        "    data.append(\n",
        "        go.Bar(\n",
        "            x=list(summary_metrics.keys()),\n",
        "            y=list(summary_metrics.values()),\n",
        "            name=title,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig = go.Figure(data=data)\n",
        "\n",
        "    # Change the bar mode\n",
        "    fig.update_layout(barmode=\"group\")\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def display_radar_plot(eval_results, title: str, metrics=None):\n",
        "    \"\"\"Plot the radar plot.\"\"\"\n",
        "    fig = go.Figure()\n",
        "    summary_metrics = eval_results.summary_metrics\n",
        "    if metrics:\n",
        "        summary_metrics = {\n",
        "            k: summary_metrics[k]\n",
        "            for k, v in summary_metrics.items()\n",
        "            if any(selected_metric in k for selected_metric in metrics)\n",
        "        }\n",
        "\n",
        "    min_val = min(summary_metrics.values())\n",
        "    max_val = max(summary_metrics.values())\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatterpolar(\n",
        "            r=list(summary_metrics.values()),\n",
        "            theta=list(summary_metrics.keys()),\n",
        "            fill=\"toself\",\n",
        "            name=title,\n",
        "        )\n",
        "    )\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        polar=dict(radialaxis=dict(visible=True, range=[min_val, max_val])),\n",
        "        showlegend=True,\n",
        "    )\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDaa2Mtsifmq"
      },
      "source": [
        "## Build and deploy a LangChain agent using Vertex AI Agent Engine's prebuilt template\n",
        "\n",
        "Build and deploy your application using LangChain, including the Gemini model and custom tools that you define.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHwShhpOitKp"
      },
      "source": [
        "### Set tools\n",
        "\n",
        "To start, create some tools the agent will need to do their job. We are just going to pretend there's a database for this Colab, but you would wire into your database or third party system for a real agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA2ZKvfeislw"
      },
      "outputs": [],
      "source": [
        "def get_product_details(product_name: str):\n",
        "    \"\"\"Gathers basic details about a product.\"\"\"\n",
        "    details = {\n",
        "        \"smartphone\": \"A cutting-edge smartphone with advanced camera features and lightning-fast processing.\",\n",
        "        \"usb charger\": \"A super fast and light usb charger\",\n",
        "        \"shoes\": \"High-performance running shoes designed for comfort, support, and speed.\",\n",
        "        \"headphones\": \"Wireless headphones with advanced noise cancellation technology for immersive audio.\",\n",
        "        \"speaker\": \"A voice-controlled smart speaker that plays music, sets alarms, and controls smart home devices.\",\n",
        "    }\n",
        "    return details.get(product_name, \"Product details not found.\")\n",
        "\n",
        "\n",
        "def get_product_price(product_name: str):\n",
        "    \"\"\"Gathers price about a product.\"\"\"\n",
        "    details = {\n",
        "        \"smartphone\": 500,\n",
        "        \"usb charger\": 10,\n",
        "        \"shoes\": 100,\n",
        "        \"headphones\": 50,\n",
        "        \"speaker\": 80,\n",
        "    }\n",
        "    return details.get(product_name, \"Product price not found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4mk5XPui4Y1"
      },
      "source": [
        "### Set the model\n",
        "\n",
        "Choose which Gemini AI model your agent will use. If you're curious about Gemini and its different capabilities, take a look at [the official documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaYeo6K2i-w1"
      },
      "outputs": [],
      "source": [
        "model = \"gemini-2.0-flash\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNlAY9cojEWz"
      },
      "source": [
        "### Assemble the agent\n",
        "\n",
        "To create a LangChain agent using [Vertex AI Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/deploy), use the LangchainAgent class. This class helps you quickly get an agent running with a standard template. Think of it as a shortcut for building agents – you don't have to start from scratch. The LangchainAgent handles the basic structure and initial configuration, allowing you to get right into using the agent.\n",
        "\n",
        "> Note the additional parameter `agent_executor_kwargs` which would allow to return tool calls made by agent so you can evaluate them.\n",
        "\n",
        "The Vertex AI Gen AI Evaluation works directly with 'Queryable' agents (like in this case), and also lets you add your own custom functions with a specific structure (signature)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAFdi7SujGP8"
      },
      "outputs": [],
      "source": [
        "local_1p_agent = LangchainAgent(\n",
        "    model=model,\n",
        "    tools=[get_product_details, get_product_price],\n",
        "    agent_executor_kwargs={\"return_intermediate_steps\": True},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HGcs6PVjRj_"
      },
      "source": [
        "### Test the local agent\n",
        "\n",
        "Query your agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGb58OJkjUs9"
      },
      "outputs": [],
      "source": [
        "response = local_1p_agent.query(input=\"Get product details for shoes\")\n",
        "display(Markdown(response[\"output\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INqf60zPWP6L"
      },
      "outputs": [],
      "source": [
        "response = local_1p_agent.query(input=\"Get product price for shoes\")\n",
        "display(Markdown(response[\"output\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP5g16W1rzMI"
      },
      "source": [
        "### Deploy the local agent to Vertex AI Agent Engine\n",
        "\n",
        "To deploy the local agent on Vertex AI Agent Engine, you can use the `create` method by passing the agent and some specify dependencies (`requirements` for external PyPI packages and `extra_packages` for local packages ).\n",
        "\n",
        "Look at [Deploy the application](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/deploy) documentation page to learn more.  \n",
        "\n",
        "> The agent deployment on Vertex AI Agent Engine would require ~ 10 mins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPNpD676r6T2"
      },
      "outputs": [],
      "source": [
        "remote_1p_agent = agent_engines.create(\n",
        "    local_1p_agent,\n",
        "    requirements=[\n",
        "        \"google-cloud-aiplatform[agent_engines,langchain]\",\n",
        "        \"langchain_google_vertexai\",\n",
        "        \"cloudpickle==3.0.0\",\n",
        "        \"pydantic>=2.10\",\n",
        "        \"requests\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjZMd82vHRh3"
      },
      "source": [
        "### Test the remote agent\n",
        "\n",
        "Query your remote agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSCznbhbHRh3"
      },
      "outputs": [],
      "source": [
        "response = remote_1p_agent.query(input=\"Get product details for shoes\")\n",
        "display(Markdown(response[\"output\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGPePsorpUl"
      },
      "source": [
        "## Evaluating an agent with Vertex AI Gen AI Evaluation\n",
        "\n",
        "When working with AI agents, it's important to keep track of their performance and how well they're working. You can look at this in two main ways: **monitoring** and **observability**.\n",
        "\n",
        "Monitoring focuses on how well your agent is performing specific tasks:\n",
        "\n",
        "* **Single Tool Selection**: Is the agent choosing the right tools for the job?\n",
        "\n",
        "* **Multiple Tool Selection (or Trajectory)**: Is the agent making logical choices in the order it uses tools?\n",
        "\n",
        "* **Response generation**: Is the agent's output good, and does it make sense based on the tools it used?\n",
        "\n",
        "Observability is about understanding the overall health of the agent:\n",
        "\n",
        "* **Latency**: How long does it take the agent to respond?\n",
        "\n",
        "* **Failure Rate**: How often does the agent fail to produce a response?\n",
        "\n",
        "Vertex AI Gen AI Evaluation service helps you to assess all of these aspects both while you are prototyping the agent or after you deploy it in production. It provides [pre-built evaluation criteria and metrics](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval) so you can see exactly how your agents are doing and identify areas for improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e43229f3ad4f"
      },
      "source": [
        "### Prepare Agent Evaluation dataset\n",
        "\n",
        "To evaluate your AI agent using the Vertex AI Gen AI Evaluation service, you need a specific dataset depending on what aspects you want to evaluate of your agent.  \n",
        "\n",
        "This dataset should include the prompts given to the agent. It can also contain the ideal or expected response (ground truth) and the intended sequence of tool calls the agent should take (reference trajectory) representing the sequence of tools you expect agent calls for each given prompt.\n",
        "\n",
        "> Optionally, you can provide both generated responses and predicted trajectory (**bring-your-own-dataset scenario**).\n",
        "\n",
        "Below you have an example of dataset you might have with a customer support agent with user prompt and the reference trajectory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFf8uTdUiDt3"
      },
      "outputs": [],
      "source": [
        "eval_data = {\n",
        "    \"prompt\": [\n",
        "        \"Get price for smartphone\",\n",
        "        \"Get product details and price for headphones\",\n",
        "        \"Get details for usb charger\",\n",
        "        \"Get product details and price for shoes\",\n",
        "        \"Get product details for speaker?\",\n",
        "    ],\n",
        "    \"reference_trajectory\": [\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_price\",\n",
        "                \"tool_input\": {\"product_name\": \"smartphone\"},\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_details\",\n",
        "                \"tool_input\": {\"product_name\": \"headphones\"},\n",
        "            },\n",
        "            {\n",
        "                \"tool_name\": \"get_product_price\",\n",
        "                \"tool_input\": {\"product_name\": \"headphones\"},\n",
        "            },\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_details\",\n",
        "                \"tool_input\": {\"product_name\": \"usb charger\"},\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_details\",\n",
        "                \"tool_input\": {\"product_name\": \"shoes\"},\n",
        "            },\n",
        "            {\"tool_name\": \"get_product_price\", \"tool_input\": {\"product_name\": \"shoes\"}},\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"tool_name\": \"get_product_details\",\n",
        "                \"tool_input\": {\"product_name\": \"speaker\"},\n",
        "            }\n",
        "        ],\n",
        "    ],\n",
        "}\n",
        "\n",
        "eval_sample_dataset = pd.DataFrame(eval_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQEI1EcfvFHb"
      },
      "source": [
        "Print some samples from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjsonqWWvIvE"
      },
      "outputs": [],
      "source": [
        "display_dataframe_rows(eval_sample_dataset, num_rows=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4CvBuf1afHG"
      },
      "source": [
        "### Single tool usage evaluation\n",
        "\n",
        "After you've set your AI agent and the evaluation dataset, you start evaluating if the agent is choosing the correct single tool for a given task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rS5GGKHd5bx"
      },
      "source": [
        "#### Set single tool usage metrics\n",
        "\n",
        "The `trajectory_single_tool_use` metric in Vertex AI Gen AI Evaluation gives you a quick way to evaluate whether your agent is using the tool you expect it to use, regardless of any specific tool order. It's a basic but useful way to start evaluating if the right tool was used at some point during the agent's process.\n",
        "\n",
        "To use the `trajectory_single_tool_use` metric, you need to set what tool should have been used for a particular user's request. For example, if a user asks to \"send an email\", you might expect the agent to use an \"send_email\" tool, and you'd specify that tool's name when using this metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xixvq8dwd5by"
      },
      "outputs": [],
      "source": [
        "single_tool_usage_metrics = [TrajectorySingleToolUse(tool_name=\"get_product_price\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktKZoT2Qd5by"
      },
      "source": [
        "#### Run an evaluation task\n",
        "\n",
        "To run the evaluation, you initiate an `EvalTask` using the pre-defined dataset (`eval_sample_dataset`) and metrics (`single_tool_usage_metrics` in this case) within an experiment. Then, you run the evaluation using the remote agent and assigns a unique identifier to this specific evaluation run, storing and visualizing the evaluation results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaMf9dqzySE6"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_RUN = f\"single-metric-eval-{get_id()}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRv43fDcd5by"
      },
      "outputs": [],
      "source": [
        "single_tool_call_eval_task = EvalTask(\n",
        "    dataset=eval_sample_dataset,\n",
        "    metrics=single_tool_usage_metrics,\n",
        "    experiment=EXPERIMENT_NAME,\n",
        ")\n",
        "\n",
        "single_tool_call_eval_result = single_tool_call_eval_task.evaluate(\n",
        "    runnable=remote_1p_agent, experiment_run_name=EXPERIMENT_RUN\n",
        ")\n",
        "\n",
        "display_eval_report(single_tool_call_eval_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o5BjSTFKVMS"
      },
      "source": [
        "#### Visualize evaluation results\n",
        "\n",
        "Use some helper functions to visualize a sample of evaluation result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Jopzw83k14w"
      },
      "outputs": [],
      "source": [
        "display_dataframe_rows(single_tool_call_eval_result.metrics_table, num_rows=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlujdJpu5Kn6"
      },
      "source": [
        "### Trajectory Evaluation\n",
        "\n",
        "After evaluating the agent's ability to select the single most appropriate tool for a given task, you generalize the evaluation by analyzing the tool sequence choices with respect to the user input (trajectory). This assesses whether the agent not only chooses the right tools but also utilizes them in a rational and effective order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s-nHdDJneHM"
      },
      "source": [
        "#### Set trajectory metrics\n",
        "\n",
        "To evaluate agent's trajectory, Vertex AI Gen AI Evaluation provides several ground-truth based metrics:\n",
        "\n",
        "* `trajectory_exact_match`: identical trajectories (same actions, same order)\n",
        "\n",
        "* `trajectory_in_order_match`: reference actions present in predicted trajectory, in order (extras allowed)\n",
        "\n",
        "* `trajectory_any_order_match`: all reference actions present in predicted trajectory (order, extras don't matter).\n",
        "\n",
        "* `trajectory_precision`: proportion of predicted actions present in reference\n",
        "\n",
        "* `trajectory_recall`: proportion of reference actions present in predicted.  \n",
        "\n",
        "All metrics score 0 or 1, except `trajectory_precision` and `trajectory_recall` which range from 0 to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c32WIS95neHN"
      },
      "outputs": [],
      "source": [
        "trajectory_metrics = [\n",
        "    \"trajectory_exact_match\",\n",
        "    \"trajectory_in_order_match\",\n",
        "    \"trajectory_any_order_match\",\n",
        "    \"trajectory_precision\",\n",
        "    \"trajectory_recall\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF3jhTH3neHN"
      },
      "source": [
        "#### Run an evaluation task\n",
        "\n",
        "Submit an evaluation by running `evaluate` method of the new `EvalTask`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOdS7TJUneHN"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_RUN = f\"trajectory-{get_id()}\"\n",
        "\n",
        "trajectory_eval_task = EvalTask(\n",
        "    dataset=eval_sample_dataset, metrics=trajectory_metrics, experiment=EXPERIMENT_NAME\n",
        ")\n",
        "\n",
        "trajectory_eval_result = trajectory_eval_task.evaluate(runnable=remote_1p_agent)\n",
        "\n",
        "display_eval_report(trajectory_eval_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBiUI3LyLBtj"
      },
      "source": [
        "#### Visualize evaluation results\n",
        "\n",
        "Print and visualize a sample of evaluation results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLVRdN5llA0h"
      },
      "outputs": [],
      "source": [
        "display_dataframe_rows(trajectory_eval_result.metrics_table, num_rows=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrxM5sMZYXHP"
      },
      "outputs": [],
      "source": [
        "plot_bar_plot(\n",
        "    trajectory_eval_result,\n",
        "    title=\"Trajectory Metrics\",\n",
        "    metrics=[f\"{metric}/mean\" for metric in trajectory_metrics],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8TipU2akHEd"
      },
      "source": [
        "### Evaluate final response\n",
        "\n",
        "Similar to model evaluation, you can evaluate the final response of the agent using Vertex AI Gen AI Evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeK-py7ykkDN"
      },
      "source": [
        "#### Set response metrics\n",
        "\n",
        "After agent inference, Vertex AI Gen AI Evaluation provides several metrics to evaluate generated responses. You can use computation-based metrics to compare the response to a reference (if needed) and using existing or custom model-based metrics to determine the quality of the final response.\n",
        "\n",
        "Check out the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval) to learn more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyGHGgeVklvz"
      },
      "outputs": [],
      "source": [
        "response_metrics = [\"safety\", \"coherence\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaBJWcg1kn55"
      },
      "source": [
        "#### Run an evaluation task\n",
        "\n",
        "To evaluate agent's generated responses, use the `evaluate` method of the EvalTask class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRb2EC_hknSD"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_RUN = f\"response-{get_id()}\"\n",
        "\n",
        "response_eval_task = EvalTask(\n",
        "    dataset=eval_sample_dataset, metrics=response_metrics, experiment=EXPERIMENT_NAME\n",
        ")\n",
        "\n",
        "response_eval_result = response_eval_task.evaluate(runnable=remote_1p_agent)\n",
        "\n",
        "display_eval_report(response_eval_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtewTwiwg9qH"
      },
      "source": [
        "#### Visualize evaluation results\n",
        "\n",
        "\n",
        "Print new evaluation result sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZODTRuq2lF75"
      },
      "outputs": [],
      "source": [
        "display_dataframe_rows(response_eval_result.metrics_table, num_rows=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a4e033321ad"
      },
      "source": [
        "## Cleaning up\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox2I3UfRlTOd"
      },
      "outputs": [],
      "source": [
        "delete_experiment = True\n",
        "delete_remote_agent = True\n",
        "\n",
        "if delete_experiment:\n",
        "    try:\n",
        "        experiment = aiplatform.Experiment(EXPERIMENT_NAME)\n",
        "        experiment.delete(delete_backing_tensorboard_runs=True)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "if delete_remote_agent:\n",
        "    try:\n",
        "        remote_1p_agent.delete()\n",
        "    except Exception as e:\n",
        "        print(e)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}