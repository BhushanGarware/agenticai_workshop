{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Getting Started\n",
        "This notebook provides a basic introduction to building a LangGraph, a framework for creating complex conversational AI agents. It covers a basic conversational loop where the assistant generates a response, a condition checks if a tool is needed, and if so, a tool is invoked, and its output is fed back to the assistant. The visualization helps in understanding this flow and the decision points within the graph."
      ],
      "metadata": {
        "id": "mGY5AxnuftJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Required Packages"
      ],
      "metadata": {
        "id": "0IY6qAJ7f3_C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPqvZn4tuVSM"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --user --quiet \"google-cloud-aiplatform[evaluation, langchain, reasoningengine]\" \\\n",
        "    \"langchain_google_vertexai\" \\\n",
        "    \"langgraph\" \\\n",
        "    \"cloudpickle==3.0.0\" \\\n",
        "    \"pydantic>=2.10\" \\\n",
        "    \"requests\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restart current runtime"
      ],
      "metadata": {
        "id": "e7PBO9qEgEiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "AcHuE4FCuf5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticate your notebook environment (Colab only)"
      ],
      "metadata": {
        "id": "4lE-xGqxgKBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "wGLeIGc2utU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK"
      ],
      "metadata": {
        "id": "mgZtequ5ga1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import os\n",
        "import vertexai\n",
        "\n",
        "PROJECT_ID =\"\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "LOCATION =\"us-central1\"\n",
        "\n",
        "\n",
        "vertexai.init(project=PROJECT_ID,location=LOCATION)"
      ],
      "metadata": {
        "id": "MAuT9k10uocn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "zACMDEHYgk5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.prebuilt import tools_condition\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from IPython.display import Image, display\n",
        "from langchain_google_vertexai import ChatVertexAI"
      ],
      "metadata": {
        "id": "P-gP98RYu0gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define model"
      ],
      "metadata": {
        "id": "SCuzrUDzgn7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm=ChatVertexAI(model_name=\"gemini-1.5-flash-001\")"
      ],
      "metadata": {
        "id": "GXTZR4Isu6UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Python functions (tools)"
      ],
      "metadata": {
        "id": "yvwLTMAAgwGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "# This will be a tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a / b\n",
        "\n",
        "tools = [add, multiply, divide]"
      ],
      "metadata": {
        "id": "bak3c-3DvHzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For this ipynb we set parallel tool calling to false as math generally is done sequentially, and this time we have 3 tools that can do math\n",
        "# the OpenAI model specifically defaults to parallel tool calling for efficiency, see https://python.langchain.com/docs/how_to/tool_calling_parallel/\n",
        "# play around with it and see how the model behaves with math equations!\n",
        "llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)"
      ],
      "metadata": {
        "id": "eW-TyxLJvJrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "# The message state acts as the memory or the history of interactions within your LangGraph application.\n",
        "# It's the primary way different nodes (representing steps or agents) in the graph communicate and share information.\n",
        "\n",
        "\n",
        "# System message\n",
        "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
        "\n",
        "# Node\n",
        "def assistant(state: MessagesState):\n",
        "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
      ],
      "metadata": {
        "id": "JxN3XNZRvXAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph\n",
        "builder = StateGraph(MessagesState)\n",
        "\n",
        "# Define nodes: these do the work\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Define edges: these determine how the control flow moves\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    tools_condition,\n",
        ")\n",
        "\n",
        "# The tools_condition being used is a pre-built function provided by LangGraph.\n",
        "# This function is designed to specifically handle the logic of determining whether,\n",
        "# the output of a language model (\"assistant\" node) indicates a need to use a tool.\n",
        "\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "react_graph = builder.compile()\n",
        "\n",
        "# Show\n",
        "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "uB7nSJCfvtbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"Add 3 and 4. Multiply the output by 2. Divide the output by 5\")]\n",
        "messages = react_graph.invoke({\"messages\": messages})"
      ],
      "metadata": {
        "id": "K11pXsjNvgEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "id": "0SfPE5GXxjiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n-TSDyZExnEh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}