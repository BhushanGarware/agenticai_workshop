{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Getting Started\n",
        "This notebook demonstrates a fundamental multi-agent workflow built using LangGraph. We will construct a graph with a supervisor node that dynamically routes tasks to specialized agent nodes: a researcher and a calculator. The core of this workflow is managed by a LangGraph StateGraph, where the state evolves as the agents interact and perform their designated roles.\n"
      ],
      "metadata": {
        "id": "MZV2szuArHpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Required Packages"
      ],
      "metadata": {
        "id": "gsXuT5EUrNJG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiLSb9Am1Jpb"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --user --quiet \"google-cloud-aiplatform[evaluation, langchain, reasoningengine]\" \\\n",
        "    \"langchain_google_vertexai\" \\\n",
        "    \"langgraph\" \\\n",
        "    \"cloudpickle==3.0.0\" \\\n",
        "    \"pydantic>=2.10\" \\\n",
        "    \"requests\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restart current runtime"
      ],
      "metadata": {
        "id": "v2U3L1BtrRPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "c2HIb1Z83Ayf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticate your notebook environment (Colab only)"
      ],
      "metadata": {
        "id": "pyIAug_5rVmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "n2i0OKrM2WPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI"
      ],
      "metadata": {
        "id": "8-t_IfgwrcuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import os\n",
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "LOCATION =\"us-central1\"\n",
        "\n",
        "vertexai.init(project=PROJECT_ID,location=LOCATION,)"
      ],
      "metadata": {
        "id": "qvnijAuJ2xYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "MQRXJxKyxk97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "from langgraph.prebuilt import tools_condition\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from IPython.display import Image, display\n",
        "from langchain_google_vertexai import ChatVertexAI\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from typing import TypedDict, List, Tuple, Dict, Any\n",
        "from typing import Literal\n",
        "from langgraph.graph import MessagesState,StateGraph,END,START\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.types import Command\n",
        "from vertexai.generative_models import (GenerationConfig,GenerativeModel,Tool, grounding)"
      ],
      "metadata": {
        "id": "5Z1DB6QQryCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Search Tool"
      ],
      "metadata": {
        "id": "KUq2o0Ewxto5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def search_tool(prompt: str)->str:\n",
        "    \"\"\"\n",
        "    Use Google Search for grounding.\n",
        "\n",
        "    This function utilizes the Google Search tool to retrieve information relevant to the given prompt.\n",
        "    It employs dynamic retrieval to adjust the search threshold based on relevance.\n",
        "    The function generates content using the provided prompt and tools, and returns the response.\n",
        "    \"\"\"\n",
        "    model = GenerativeModel(\"gemini-1.5-pro\")\n",
        "    tool = Tool.from_google_search_retrieval(\n",
        "        grounding.GoogleSearchRetrieval(\n",
        "            # Optional: For Dynamic Retrieval\n",
        "            dynamic_retrieval_config=grounding.DynamicRetrievalConfig(\n",
        "                mode=grounding.DynamicRetrievalConfig.Mode.MODE_DYNAMIC,\n",
        "                dynamic_threshold=0.7,\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    response = model.generate_content(\n",
        "        prompt,\n",
        "        tools=[tool],\n",
        "        generation_config=GenerationConfig(\n",
        "            temperature=0.0,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return(response.text)\n"
      ],
      "metadata": {
        "id": "DOUOT9zu4AYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test the tool\n",
        "search_tool(\"Which is the capital city of India ?\")"
      ],
      "metadata": {
        "id": "-V6C84QUwTXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define model"
      ],
      "metadata": {
        "id": "ymfNF05nyF7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm=ChatVertexAI(model_name=\"gemini-2.0-flash-001\")"
      ],
      "metadata": {
        "id": "lKJCcxPA4FQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Research Agent"
      ],
      "metadata": {
        "id": "0nbG8xm1yJFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'create_react_agent' function in LangGraph's pre-built templates provides\n",
        "# a streamlined way to create a ReAct agent. ReAct (Reasoning and Acting) is a popular framework\n",
        "# for building agents that can interact with tools.\n",
        "# This function encapsulates the core components and logic needed to set up such an agent.\n",
        "research_agent = create_react_agent(llm, tools=[search_tool], prompt=\"You are a researcher. DO NOT do any math.\")"
      ],
      "metadata": {
        "id": "bbb25-QuyKZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Python functions (tools)"
      ],
      "metadata": {
        "id": "RnWJYwduyFIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "# This will be a tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "def divide(a: int, b: int) -> float:\n",
        "    \"\"\"Divide a and b.\n",
        "\n",
        "    Args:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a / b\n",
        "\n",
        "my_tools = [add, multiply, divide]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OauPfUsv4PiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Math Agent"
      ],
      "metadata": {
        "id": "bRfeEtjizFiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "math_agent = create_react_agent(llm, tools=my_tools)"
      ],
      "metadata": {
        "id": "itVP2CrezDS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Building the Graph"
      ],
      "metadata": {
        "id": "F8LqFDJhzO2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "members = [\"researcher\", \"calculator\"]\n",
        "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
        "# and decides when the work is completed\n",
        "options = members + [\"FINISH\"]\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are a supervisor tasked with managing a conversation between the\"\n",
        "    f\" following workers: {members}. Given the following user request,\"\n",
        "    \"Respond FINISH if answer is correct\"\n",
        "    \"Respond with the worker to act next. \"\n",
        "\n",
        ")\n",
        "\n",
        "class Router(TypedDict):\n",
        "    \"\"\"Worker to route to next. Once task is done, route to FINISH.\"\"\"\n",
        "    next: Literal[\"researcher\", \"calculator\", \"FINISH\"]\n",
        "\n",
        "\n",
        "class State(MessagesState):\n",
        "    next: str"
      ],
      "metadata": {
        "id": "5RFAOLsz4nnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Supervisor Node"
      ],
      "metadata": {
        "id": "_XzexPdTzYHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def supervisor_node(state: State) -> Command[Literal[\"researcher\", \"calculator\", \"__end__\"]]:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "    ] + state[\"messages\"]\n",
        "    response = llm.with_structured_output(Router).invoke(messages)\n",
        "    print(\"\\n---This is what superviser see:\")\n",
        "    print(response)\n",
        "    print(\"\\n-----\")\n",
        "    goto = response[\"next\"]\n",
        "    if goto == \"FINISH\":\n",
        "        goto = \"__end__\"\n",
        "\n",
        "    if (len(state[\"messages\"]) >= 2):\n",
        "      goto = \"__end__\"\n",
        "\n",
        "    return Command(goto=goto, update={\"next\": goto})\n"
      ],
      "metadata": {
        "id": "ggOgBjhZzY3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Research Node"
      ],
      "metadata": {
        "id": "FjhDQNBO3aed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def research_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
        "    result = research_agent.invoke(state)\n",
        "    return Command(\n",
        "        update={\n",
        "            \"messages\": [\n",
        "                HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")\n",
        "            ]\n",
        "        },\n",
        "        goto=\"supervisor\",\n",
        "    )"
      ],
      "metadata": {
        "id": "8VmpKGai47L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Calculator Node"
      ],
      "metadata": {
        "id": "HTbnKz773evN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculator_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
        "    result = math_agent.invoke(state)\n",
        "    return Command(\n",
        "        update={\n",
        "            \"messages\": [\n",
        "                HumanMessage(content=result[\"messages\"][-1].content, name=\"calculator\")\n",
        "            ]\n",
        "        },\n",
        "        goto=\"supervisor\",\n",
        "    )"
      ],
      "metadata": {
        "id": "q3zkA4Ut5CR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Grpah Structure"
      ],
      "metadata": {
        "id": "cn4SKYa_36Rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(State)\n",
        "builder.add_edge(START, \"supervisor\")\n",
        "builder.add_node(\"supervisor\", supervisor_node)\n",
        "builder.add_node(\"researcher\", research_node)\n",
        "builder.add_node(\"calculator\",calculator_node)\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "wGHGVJoY5rju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Image\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "-MvWGx0W6LUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute the query"
      ],
      "metadata": {
        "id": "tM7gghHe4Ajf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for s in graph.stream(\n",
        "    {\"messages\": [(\"user\", \"What is capital city of India? \")]}, subgraphs=True):\n",
        "    print(s)\n",
        "    print(\"----\")"
      ],
      "metadata": {
        "id": "32eSzZj86PmT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
